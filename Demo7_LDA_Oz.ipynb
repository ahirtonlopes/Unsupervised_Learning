{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e68dd8f-aab6-4969-be1c-85e3958eb5ca",
      "metadata": {
        "id": "4e68dd8f-aab6-4969-be1c-85e3958eb5ca"
      },
      "source": [
        "# Aula 5 - Modelagem de T√≥picos com Latent Dirichlet Allocation (LDA)\n",
        "\n",
        "Prof. Dr. Ahirton Lopes\n",
        "\n",
        "### Demo 7 - Algoritmo LDA - Aprendendo Modelagem de T√≥picos com \"O M√°gico de Oz\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b7c30d-baa6-4186-a98a-f090f82e38a2",
      "metadata": {
        "id": "02b7c30d-baa6-4186-a98a-f090f82e38a2"
      },
      "source": [
        "![The Wonderful Wizard of Oz](https://www.gutenberg.org/files/55/55-h/images/cover.jpg)\n",
        "\n",
        "> The Wonderful Wizard of Oz. Acesso via https://www.gutenberg.org/ebooks/55"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adf84bb5-fbd4-49a2-a75f-a626feb9bb38",
      "metadata": {
        "id": "adf84bb5-fbd4-49a2-a75f-a626feb9bb38"
      },
      "source": [
        "Em nossa aula de n√∫mero 5 apresentei o que √© o problema de \"Modelagem de T√≥picos\" e como as t√©cnicas nesse campo podem ser utilizadas para solu√ß√£o de problemas de mundo real.\n",
        "\n",
        "Modelagem de T√≥picos tem sido a estrat√©gia principal nos √∫ltimos anos para o problema de descoberta de t√≥picos a partir de um grande *corpus* textual.\n",
        "\n",
        "Vamos come√ßar entendo mais detalhes do nosso problema: Quais s√£o os t√≥picos? Como eles s√£o definidos? N√≥s definimos ou o computador? O que √© um corpus grande? Quantos documentos precisamos?\n",
        "\n",
        "Vamos come√ßar com um *grande corpus de documentos de texto*. Normalmente, ter√≠amos dois documentos üìÑ, cinco documentos üìÑ, dez milh√µes de documentos üìÑ, os quais podem ser considerados nosso corpus. Sim, at√© mesmo 1 documento üìÑ pode ser usado para modelagem de t√≥picos. Portanto, definir um *grande corpus de documentos de texto* pode ser subjetivo.\n",
        "\n",
        "Neste notebook voc√™ ter√° uma experi√™ncia com o algoritmo LDA (*Latent Dirichlet Allocation*) .\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e206b8d-bced-4d35-a346-8e1c45441936",
      "metadata": {
        "id": "7e206b8d-bced-4d35-a346-8e1c45441936"
      },
      "source": [
        "## O que √© o Latent Dirichlet Allocation (LDA)?\n",
        "\n",
        "LDA √© um modelo de Aprendizado N√£o Supervisionado."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fde1589-0694-4185-a22f-af82265f034b",
      "metadata": {
        "id": "8fde1589-0694-4185-a22f-af82265f034b"
      },
      "source": [
        ":::{observa√ß√£o}\n",
        "\n",
        "Modelagem de t√≥picos a partir de documentos üìÑ\n",
        "\n",
        "* Aprendizado Supervisionado - Nossos documentos üìÑ s√£o pr√©-etiquetados com o(s) tema(s) indicado(s). Podemos ent√£o treinar üèãÔ∏è e testar üß™ (e tamb√©m, voc√™ pode incluir a valida√ß√£o). **Geralmente** isso √© dividido:\n",
        " * treinamento üèãÔ∏è 80%\n",
        " *testes üß™ 20%.\n",
        "\n",
        "\n",
        "* Aprendizado N√£o Supervisionado - Os dados n√£o s√£o rotulados. Portanto, n√£o temos ideia de quais s√£o os t√≥picos de antem√£o. Dito isto, podemos (e iremos) definir o *n√∫mero de t√≥picos*.\n",
        "\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6bab6c7-8078-483b-a84a-dec916201024",
      "metadata": {
        "id": "d6bab6c7-8078-483b-a84a-dec916201024"
      },
      "source": [
        "Ent√£o, voltando √†s nossas perguntas originais:\n",
        "\n",
        "* Quais s√£o os t√≥picos?\n",
        "\n",
        " * Os t√≥picos ter√£o um n√∫mero X de conjuntos de termos (definimos este X) que ter√£o (poder√£o) ter um tema comum.\n",
        "\n",
        "*Como eles s√£o definidos?\n",
        "\n",
        " * √â a isso que chegaremos neste *notebook*.\n",
        "\n",
        "* N√≥s definimos ou o computador?\n",
        "\n",
        " * O LDA n√£o √© supervisionado, por isso definimos a quantidade de t√≥picos. O computador fornece os pr√≥prios t√≥picos.\n",
        "\n",
        "* O que √© um corpus grande? e Quantos documentos precisamos?\n",
        "\n",
        " * Um pouco subjetivo aqui. H√° uma *√≥tima* discuss√£o sobre isso por Tang et al. {cite:p}`tang2014understanding` em rela√ß√£o a isso. Se voc√™ tiver oportunidade, leia todos os pontos, mas para resumir:\n",
        "\n",
        " * O n√∫mero de documentos √© importante, mas em algum momento, aumentar o n√∫mero n√£o garante melhores resultados. Mesmo a amostragem de 1.000 artigos de 1.000.000 de artigos poderia resultar nos mesmos, se n√£o melhores, resultados do que 1.000.000 de documentos.\n",
        "\n",
        " * O tamanho dos documentos tamb√©m desempenha um papel importante, portanto os documentos n√£o devem ser curtos. Documentos maiores podem ser amostrados e receber novamente o mesmo resultado desejado.\n",
        "\n",
        " Ref. https://proceedings.mlr.press/v32/tang14.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faf3ca1c-1dc5-4f33-acfc-9e26c3f2c73b",
      "metadata": {
        "id": "faf3ca1c-1dc5-4f33-acfc-9e26c3f2c73b"
      },
      "source": [
        "Uma das melhores representa√ß√µes do que √© LDA e como utiliz√°-lo pode ser encontrada no trabalho de Blei *Modelos de t√≥picos probabil√≠sticos* {cite:p}`blei2012probabilistic`.\n",
        "\n",
        "![As intui√ß√µes por tr√°s da aloca√ß√£o latente de Dirichlet](http://deliveryimages.acm.org/10.1145/2140000/2133826/figs/f1.jpg)\n",
        "\"Figura 1. As intui√ß√µes por tr√°s da aloca√ß√£o latente de Dirichlet. Assumimos que existe um certo n√∫mero de \"t√≥picos\", que s√£o distribui√ß√µes sobre palavras, para toda a cole√ß√£o (extrema esquerda). Sup√µe-se que cada documento seja gerado da seguinte forma. Primeiro escolha um distribui√ß√£o pelos t√≥picos (o histograma √† direita); ent√£o, para cada palavra, escolha uma atribui√ß√£o de t√≥pico (as moedas coloridas) e escolha a palavra do t√≥pico correspondente. Os t√≥picos e as atribui√ß√µes de t√≥pico nesta figura s√£o ilustrativos ‚Äì eles n√£o s√£o adequados. a partir de dados reais. {cite:p}`blei2012probabilistic` (P√°gina 3)\"\n",
        "\n",
        "![Infer√™ncia real com LDA](https://deliveryimages.acm.org/10.1145/2140000/2133826/figs/f2.jpg)\n",
        "\"Figura 2. Infer√™ncia real com LDA. Ajustamos um modelo LDA de 100 t√≥picos a 17.000 artigos da revista Science. √Ä esquerda est√£o as propor√ß√µes de t√≥picos inferidas para o artigo de exemplo na Figura 1. √Ä direita est√£o as 15 palavras mais frequentes de os t√≥picos mais frequentes encontrados neste artigo. {cite:p}`blei2012probabilistic` (P√°gina 4)\"\n",
        "\n",
        "Ref. https://www.cs.columbia.edu/~blei/papers/Blei2012.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4dbe6f0-e011-4ca9-b564-de59f28159b0",
      "metadata": {
        "id": "c4dbe6f0-e011-4ca9-b564-de59f28159b0"
      },
      "source": [
        "## Hands on: Vamos testar um exemplo?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e9a02d3-1995-4ab1-949b-c0d548a6dd2d",
      "metadata": {
        "id": "0e9a02d3-1995-4ab1-949b-c0d548a6dd2d"
      },
      "source": [
        "Para nosso exemplo, usaremos um subconjunto de livros de L. Frank Baum que fazem parte de dom√≠nio p√∫blico.\n",
        "\n",
        "* O Maravilhoso M√°gico de Oz\n",
        " * https://www.gutenberg.org/files/55/55-h/55-h.htm\n",
        "\n",
        "* A Maravilhosa Terra de Oz\n",
        " * https://www.gutenberg.org/files/54/54-h/54-h.htm\n",
        "\n",
        "* Ozma de Oz\n",
        " * https://www.gutenberg.org/files/33361/33361-h/33361-h.htm\n",
        "\n",
        "* Dorothy e o M√°gico de Oz\n",
        " * https://www.gutenberg.org/files/22566/22566-h/22566-h.htm\n",
        "\n",
        "* O caminho para Oz\n",
        " * https://www.gutenberg.org/files/26624/26624-h/26624-h.htm#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1729ea5-325a-4474-bda3-c44d6aa8e05d",
      "metadata": {
        "id": "c1729ea5-325a-4474-bda3-c44d6aa8e05d"
      },
      "source": [
        "Os livros s√£o todos de dom√≠nio p√∫blico e o HTML pode ser encontrado em https://www.gutenberg.org/.\n",
        "Veremos um exemplo de como obter o texto do livro usando Python. Observe que esta n√£o ser√° a maneira ideal de fazer isso, mas esperamos poder deixar o processo claro para voc√™ tentar com outros livros ou manuscritos.\n",
        "\n",
        "### Obtenha o HTML do livro\n",
        "\n",
        "Usaremos duas bibliotecas para isso; um √© chamado de padr√£o para Python.\n",
        "\n",
        "```p√≠ton\n",
        "importar URLlib\n",
        "```\n",
        "a outra √© uma das nossas favoritas, chamada BeautifulSoup {cite:p}`BeautifulSoup`.\n",
        "\n",
        "```p√≠ton\n",
        "da importa√ß√£o bs4 BeautifulSoup\n",
        "```\n",
        "\n",
        "urllib obter√° o documento e o BeautifulSoup facilitar√° a an√°lise.\n",
        "\n",
        "Ref. https://beautiful-soup-4.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc077908-6bad-4e1c-80e1-161ba6a4458f",
      "metadata": {
        "id": "fc077908-6bad-4e1c-80e1-161ba6a4458f"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/55/55-h/55-h.htm\"\n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "827e1e3a-a17c-475c-967b-9d99a63e4460",
      "metadata": {
        "id": "827e1e3a-a17c-475c-967b-9d99a63e4460"
      },
      "source": [
        "Aqui removemos qualquer CSS (estilo) ou JavaScript (script)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "043de121-b7f6-4ec5-bed6-88f81c577e80",
      "metadata": {
        "id": "043de121-b7f6-4ec5-bed6-88f81c577e80"
      },
      "outputs": [],
      "source": [
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90bf0a30-811c-4e30-b81b-5d798e66037f",
      "metadata": {
        "id": "90bf0a30-811c-4e30-b81b-5d798e66037f"
      },
      "source": [
        "Por fim, pegue o texto e adicione-o √† nossa lista de documentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2baa4a1-c854-48d0-ba13-8a02232ab77c",
      "metadata": {
        "id": "f2baa4a1-c854-48d0-ba13-8a02232ab77c"
      },
      "outputs": [],
      "source": [
        "text = soup.get_text()\n",
        "documents = []\n",
        "documents.append(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c513502-b0ee-4a8a-ae59-0195456bf56a",
      "metadata": {
        "id": "9c513502-b0ee-4a8a-ae59-0195456bf56a"
      },
      "source": [
        "Repetiremos esse processo para os outros quatro livros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e823b967-0720-4fd4-926f-84d013508da5",
      "metadata": {
        "id": "e823b967-0720-4fd4-926f-84d013508da5"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.gutenberg.org/files/54/54-h/54-h.htm\"\n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "text = soup.get_text()\n",
        "documents.append(text)\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/33361/33361-h/33361-h.htm\"\n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "text = soup.get_text()\n",
        "documents.append(text)\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/22566/22566-h/22566-h.htm\"\n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "text = soup.get_text()\n",
        "documents.append(text)\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/26624/26624-h/26624-h.htm\"\n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "text = soup.get_text()\n",
        "documents.append(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79da5c65-d8e6-4687-b9dd-8432035b9234",
      "metadata": {
        "id": "79da5c65-d8e6-4687-b9dd-8432035b9234"
      },
      "source": [
        "### Criando Tokens e Vocabul√°rio\n",
        "\n",
        "Agora que temos nossos livros, precisamos tokenizar as hist√≥rias por palavra e ent√£o criar um vocabul√°rio a partir desses tokens. sklearn √© uma biblioteca fant√°stica que usaremos em todo o notebook `sklearn_api`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "798109d6-534a-42cf-b0c1-724b34097cbc",
      "metadata": {
        "id": "798109d6-534a-42cf-b0c1-724b34097cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2fa8483-9f85-4c66-eccf-fe3540cbafe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m√ó\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7db0482-3d3e-4530-ac72-2a4fdb658c89",
      "metadata": {
        "id": "a7db0482-3d3e-4530-ac72-2a4fdb658c89"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()\n",
        "df = cv.fit_transform(documents)\n",
        "vocab = cv.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83a8cb3-8446-4e1b-b4e8-08f35e4ddbd6",
      "metadata": {
        "id": "b83a8cb3-8446-4e1b-b4e8-08f35e4ddbd6"
      },
      "source": [
        "Vamos dar uma olhada nos tokens e no n√∫mero de ocorr√™ncia dos tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010f147b-256f-4d79-aafb-28cb0ed3aa65",
      "metadata": {
        "id": "010f147b-256f-4d79-aafb-28cb0ed3aa65",
        "outputId": "68b29ef0-c937-4aa1-978f-6bd91f54888f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 8066)\t3198\n",
            "  (0, 6151)\t89\n",
            "  (0, 3798)\t99\n",
            "  (0, 2713)\t14\n",
            "  (0, 5457)\t976\n",
            "  (0, 8999)\t28\n",
            "  (0, 8980)\t44\n",
            "  (0, 5592)\t169\n",
            "  (0, 1343)\t119\n",
            "  (0, 3399)\t5\n",
            "  (0, 892)\t5\n",
            "  (0, 8099)\t196\n",
            "  (0, 4376)\t284\n",
            "  (0, 3329)\t354\n",
            "  (0, 8578)\t28\n",
            "  (0, 592)\t18\n",
            "  (0, 595)\t4\n",
            "  (0, 4215)\t544\n",
            "  (0, 8506)\t15\n",
            "  (0, 7633)\t19\n",
            "  (0, 550)\t1738\n",
            "  (0, 5167)\t24\n",
            "  (0, 5544)\t65\n",
            "  (0, 5663)\t2\n",
            "  (0, 9032)\t19\n",
            "  :\t:\n",
            "  (0, 5608)\t1\n",
            "  (0, 2547)\t1\n",
            "  (0, 5047)\t1\n",
            "  (0, 403)\t1\n",
            "  (0, 8784)\t1\n",
            "  (0, 2063)\t1\n",
            "  (0, 1407)\t1\n",
            "  (0, 6142)\t1\n",
            "  (0, 5050)\t1\n",
            "  (0, 3879)\t1\n",
            "  (0, 5533)\t1\n",
            "  (0, 7085)\t1\n",
            "  (0, 6138)\t1\n",
            "  (0, 4807)\t1\n",
            "  (0, 5319)\t1\n",
            "  (0, 8690)\t1\n",
            "  (0, 1840)\t1\n",
            "  (0, 5286)\t1\n",
            "  (0, 2723)\t1\n",
            "  (0, 4882)\t1\n",
            "  (0, 5810)\t1\n",
            "  (0, 3040)\t1\n",
            "  (0, 6137)\t1\n",
            "  (0, 7814)\t1\n",
            "  (0, 5330)\t1\n"
          ]
        }
      ],
      "source": [
        "print (df[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f93c6b19-5e3d-4a83-9c5d-4d6518aaf4af",
      "metadata": {
        "id": "f93c6b19-5e3d-4a83-9c5d-4d6518aaf4af"
      },
      "source": [
        "O segundo n√∫mero listado √© o n√∫mero do token, e usamos a lista de vocabul√°rio para ver qual √© a palavra real. Um exemplo seria olhar para a primeira linha.\n",
        "\n",
        "```p√≠ton\n",
        "(0, 8066) 3198\n",
        "```\n",
        "O token 8066 foi usado 3.198 vezes. O token 8066 √©:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820ce127-340f-4ebc-8437-8571a1ee23a8",
      "metadata": {
        "id": "820ce127-340f-4ebc-8437-8571a1ee23a8",
        "outputId": "227fb80f-795b-4e42-88ce-85355a9dbc46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the\n"
          ]
        }
      ],
      "source": [
        "print (vocab[8066])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "981cc43e-a82c-420d-b9b9-88ac24ec8ba7",
      "metadata": {
        "id": "981cc43e-a82c-420d-b9b9-88ac24ec8ba7"
      },
      "source": [
        "N√£o √© de surpreender que a palavra ‚Äúthe‚Äù seja usada tantas vezes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfb90a61-dd71-4bcb-afd6-4d27bf47098e",
      "metadata": {
        "id": "cfb90a61-dd71-4bcb-afd6-4d27bf47098e"
      },
      "source": [
        ":::{observa√ß√£o}\n",
        "\n",
        "Porque existem muitos termos comumente usados. Gostar√≠amos de remover essas palavras de nosso conjunto de dados. Essas palavras s√£o chamadas de *stopwords* e devem ser removidas.\n",
        "\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa51075a-6cc9-4bcb-b136-951e622a44d8",
      "metadata": {
        "id": "fa51075a-6cc9-4bcb-b136-951e622a44d8"
      },
      "source": [
        "A partir daqui, estamos no ponto em que podemos executar o LDA.\n",
        "\n",
        "Existem muito mais do que dois insumos dispon√≠veis para LDA, mas focaremos em dois.\n",
        "\n",
        "> Aqui est√£o as outras entradas: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
        "\n",
        "Os dois em que nos concentraremos s√£o:\n",
        "\n",
        "* n_components - o n√∫mero de t√≥picos, novamente, precisamos especificar isso\n",
        "* doc_topic_prior - relaciona a distribui√ß√£o do Dirichlet (o pr√≥ximo bloco de notas fornece detalhes completos sobre o Dirichlet e como ele se relaciona com o LDA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f00d8bf9-c910-49a7-b01b-f1fa3193f41a",
      "metadata": {
        "id": "f00d8bf9-c910-49a7-b01b-f1fa3193f41a",
        "outputId": "c4cce2f1-8b17-419f-9070-10735510496d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(doc_topic_prior=1, n_components=4)"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(doc_topic_prior=1, n_components=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(doc_topic_prior=1, n_components=4)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components = 4, doc_topic_prior=1)\n",
        "lda.fit(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bab10a9-a092-4d2d-a5b1-0e4c9a5ebd24",
      "metadata": {
        "id": "7bab10a9-a092-4d2d-a5b1-0e4c9a5ebd24"
      },
      "source": [
        "Imprimindo as 5 primeiras palavras por t√≥pico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "070e0159-357b-40d3-be47-da86e8c90bb5",
      "metadata": {
        "id": "070e0159-357b-40d3-be47-da86e8c90bb5",
        "outputId": "5e0d0640-ee19-490e-8cd5-3ac3cb89a91e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0\n",
            "  unlock, cruelty, passages, bruised, solemnly, portal, husband, afterwards, lain, chased\n",
            "Topic: 1\n",
            "  unlock, cruelty, passages, bruised, solemnly, portal, husband, afterwards, lain, chased\n",
            "Topic: 2\n",
            "  the, and, to, of, in, you, he, it, was, that\n",
            "Topic: 3\n",
            "  the, and, to, of, in, she, her, you, that, it\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "topic_words = {}\n",
        "n_top_words = 10\n",
        "for topic, comp in enumerate(lda.components_):\n",
        "  # para o array ndimensional \"arr\":\n",
        "  # argsort() retorna um array n-dimensional classificado de arr, chame-o de \"ranked_array\"\n",
        "  # que cont√©m os √≠ndices que classificariam arr de forma decrescente\n",
        "  # para o i-√©simo elemento em rank_array, rank_array[i] representa o √≠ndice do\n",
        "  # elemento em arr que deve estar no i-√©simo √≠ndice em rank_array\n",
        "  #ex. arr = [3,7,1,0,3,6]\n",
        "  # np.argsort(arr) -> [3, 2, 0, 4, 5, 1]\n",
        "  # word_idx cont√©m os √≠ndices em \"t√≥pico\" das principais num_top_words mais relevantes\n",
        "  # para um determinado t√≥pico ... ele √© classificado em ordem crescente no in√≠cio e depois invertido (desc. agora)\n",
        "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
        "\n",
        "    # armazena as palavras mais relevantes para o t√≥pico\n",
        "    topic_words[topic] = [vocab[i] for i in word_idx]\n",
        "\n",
        "for topic, words in topic_words.items():\n",
        "    print('Topic: %d' % topic)\n",
        "    print('  %s' % ', '.join(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11fc0063-0c69-4d16-9a72-feaef7e6a550",
      "metadata": {
        "id": "11fc0063-0c69-4d16-9a72-feaef7e6a550"
      },
      "source": [
        "Olhando para isso, n√£o temos uma imagem clara dos t√≥picos.\n",
        "\n",
        "Agora vamos remover essas palavras irrelevantes e ver como pode ser importante üßºlimpar os dadosüßº!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b1b809-062b-4cb4-b77b-846c2ef00ade",
      "metadata": {
        "id": "49b1b809-062b-4cb4-b77b-846c2ef00ade"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# podemos adicionar isso √† etapa de tokeniza√ß√£o\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "df = cv.fit_transform(documents)\n",
        "vocab = cv.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f04ea2-d8d8-4ca1-98bb-c500367599e6",
      "metadata": {
        "id": "70f04ea2-d8d8-4ca1-98bb-c500367599e6",
        "outputId": "c48d2f77-78de-42ac-937a-273a7965bd17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(doc_topic_prior=1, n_components=4)"
            ],
            "text/html": [
              "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(doc_topic_prior=1, n_components=4)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(doc_topic_prior=1, n_components=4)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components = 4, doc_topic_prior=1)\n",
        "lda.fit(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f7c95d-cb88-48fb-86f5-ba7ef49933da",
      "metadata": {
        "id": "84f7c95d-cb88-48fb-86f5-ba7ef49933da",
        "outputId": "9f09a398-5637-4637-da59-84cabd275210",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0\n",
            "  said, scarecrow, tip, horse, jack, saw, woodman, tin, pumpkinhead, mombi\n",
            "Topic: 1\n",
            "  grassy, rapid, polishing, pace, awkward, horrible, spears, pleases, gems, threatened\n",
            "Topic: 2\n",
            "  dorothy, said, scarecrow, lion, woodman, oz, great, little, tin, witch\n",
            "Topic: 3\n",
            "  dorothy, said, pg, man, little, king, ozma, wizard, asked, shaggy\n"
          ]
        }
      ],
      "source": [
        "topic_words = {}\n",
        "n_top_words = 10\n",
        "for topic, comp in enumerate(lda.components_):\n",
        "  # para o array ndimensional \"arr\":\n",
        "  # argsort() retorna um array n-dimensional classificado de arr, chame-o de \"ranked_array\"\n",
        "  # que cont√©m os √≠ndices que classificariam arr de forma decrescente\n",
        "  # para o i-√©simo elemento em rank_array, rank_array[i] representa o √≠ndice do\n",
        "  # elemento em arr que deve estar no i-√©simo √≠ndice em rank_array\n",
        "  #ex. arr = [3,7,1,0,3,6]\n",
        "  # np.argsort(arr) -> [3, 2, 0, 4, 5, 1]\n",
        "  # word_idx cont√©m os √≠ndices em \"t√≥pico\" das principais num_top_words mais relevantes\n",
        "  # para um determinado t√≥pico ... ele √© classificado em ordem crescente no in√≠cio e depois invertido (desc. agora)\n",
        "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
        "\n",
        "    # armazena as palavras mais relevantes para o t√≥pico\n",
        "    topic_words[topic] = [vocab[i] for i in word_idx]\n",
        "\n",
        "for topic, words in topic_words.items():\n",
        "    print('Topic: %d' % topic)\n",
        "    print('  %s' % ', '.join(words))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}